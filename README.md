# Decoding Dense Embeddings  
Sparse Autoencoders for Interpretable Dense Retrieval
<!-- Badges are optional. Delete if your repo is private. -->
![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/pytorch-2.x-lightgrey)
![License](https://img.shields.io/badge/license-Apache%202.0-green)

A official implementation of **Sparse Autoencoder (SAE)‚Äìbased DPR models interpretation** and **Concept-Level Sparse Retrieval (CL-SR)**, as introduced in our paper:

> **‚ÄúDecoding Dense Embeddings: Sparse Autoencoders for Interpreting and Discretizing Dense Retrieval.‚Äù**  
---

## üó∫Ô∏è Table of Contents
1. [Methodology Overview](#methodology-overview)
2. [Requirements](#requirements)
3. [SAE Training & Evaluation](#sae-training--evaluation)
4. [Generating Latent Concept Descriptions](#generating-latent-concept-descriptions)
5. [CL-SR Indexing](#cl-sr-indexing)
6. [CL-SR Inference & Benchmarking](#cl-sr-inference--benchmarking)
7. [Citation](#citation)
8. [License](#license)

---

## Methodology Overview

<div align="center">
<img src="https://github.com/user-attachments/assets/dd907ab3-e66d-49fe-930d-56351c7b7c79" width="100%" alt="Framework overview"/>
</div>

Overview of our method. We first **train a SAE** to decompose dense embeddings into latent concepts. Given a query or a passage, the SAE encoder sparsely activates latent concepts, which are **mapped to natural language descriptions** allowing human interpretability tasks. In **CL-SR, queries and passages are represented as sets of activated latent concepts.** We also demonstrate its effectiveness on subsets where traditional sparse retrieval methods struggle.

---

## Requirements

### Setup `python` environment
```
conda create -n CL-SR python==3.10.10
conda activate CL-SR
```

### Install other dependencies
```
pip install -r requirements.txt
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124
```
---
### üì• Download MSMARCO Datasets

We provide the preprocessed MSMARCO passage retrieval datasets for convenience.  
Download link: [Google Drive](https://drive.google.com/drive/folders/1YlhrcCvZNvDE1DPLdwWMS7V9Xr3kgewb?usp=sharing)

This includes:
- All 8.8M MSMARCO passages
- Train queries for SimLM embedding
- TREC-DL 2019/2020 queries and qrels
- MSMARCO Dev queries and qrels

üìÅ Please place the downloaded files under the `./data/` directory.


### üß© Generate DPR Embeddings using SimLM

To train the SAE, you need embeddings generated by a DPR model.  
We use dense embeddings from **SimLM** for all 8.8M MSMARCO passages and train queries.

To generate the embeddings, move to the `simlm/` directory and run:
```bash
bash run.sh encode
```
---

## SAE Training & Evaluation
### Train SAEs
```bash
# Train SAE (example: k = 32, MSMARCO train passages)
python sae/train_sae.py \
    --input_embs_path embs/input/ \
    --hidden-mult 32 --k 32 --batch 4096 \
    --lr 5e-5 --epochs 100 \
    --out checkpoints/
```

### Evaluate SAEs

After training the SAE, you can reconstruct the original DPR embeddings and evaluate how well the SAE preserves their information by performing dense retrieval with the reconstructed embeddings and analyzing the resulting reconstructed ranking to assess the preservation of IR performance.

```bash
# Reconstruct DPR model embeddings using a trained SAE
# model can be saved with different name
# model threshold is saved on model name as mean_of_min_act
python sae/reconstruct_embedding.py \
    --checkpoint checkpoints/sae_32_k32_mean_of_min_act_0.160438.pt \
    --input_embs_path embs/input/ \
    --recon_embs_save_path embs/recon/ \
    --model_threshold 0.160438

# Evaluate reconstructed embedding's IR performance
bash run.sh evaluate true /recon_embs_save_path/recon_embs.npy
```
---

## Generating Latent Concept Descriptions
![image](https://github.com/user-attachments/assets/9ab1c928-2b8a-4c63-9157-9a3f21aa229f)

We use GPT-4.1 Mini api to generate a natural-language description for each latent concept.
Using the top 30 most activating passages per latent, this process costs approximately $7.
With this descriptions, we can understand DPR models embeddings and the similarity scoring process.

```
# Extract latent concepts using trained SAE
# model can be saved with different name
# model threshold is saved on model name as mean_of_min_act
python sae/extract_latent_concepts.py \
    --checkpoint checkpoints/sae_32_k32_mean_of_min_act_0.160438.pt \
    --input_embs_path embs/input/ \
    --model_threshold 0.160438 \
    --latent_concepts_save_path latent_concepts \

# Generate descriptions from top-activating passages
# model can be saved with different name
python sae/generate_descriptions.py \
  --sae checkpoints/sae_32_k32_mean_of_min_act_0.160438.pt \
  --topK 30 \
  --latent_concepts sae/latent_concepts/passages_latents.jsonl \
  --passages data/msmarco_bm25_official/passages.jsonl.gz \
  --out descriptions/latent32.json
```
---

## CL-SR Indexing

Retrieval using latent concepts involves two phases:

1. **Indexing**  
   - Extract latent concepts from each passage
   - Build an inverted index over those latents

2. **Search**  
   - Extract latent concepts from each query  
   - Query the inverted index to rank passages

We‚Äôll support Anserini(https://github.com/castorini/anserini) for indexing and search. For now, use the provided custom scripts.  

```
python clsr/indexing.py \
  --passage_latents sae/latent_concepts/passages_latents.jsonl \
  --index_save_path inverted_index.pkl \
  --max_latents 24
```
---

## CL-SR Inference & Benchmarking

```
# Evaluate
python latent_retrieve.py \
    --index_path inverted_index.pkl \
    --query_latents_path sae/latent_concepts/query_latents.json \
    --output_path rank_output.txt \
    --split dev

python evaluate.py \
  --eval_data dev \
  --result_path ./results/rank_output.txt \
```
---

## Citation

---
## License

---

## License

This project is licensed under the Apache 2.0 License.\
MS MARCO data is provided under Microsoft‚Äôs non-commercial research terms.
