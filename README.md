# Decoding Dense Embeddings  
Sparse Autoencoders for Interpretable Dense Retrieval
<!-- Badges are optional. Delete if your repo is private. -->
![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/pytorch-2.x-lightgrey)
![License](https://img.shields.io/badge/license-Apache%202.0-green)

A reference implementation of **Sparse Autoencoder (SAE)‚Äìbased DPR models interpretation** and **Concept-Level Sparse Retrieval (CL-SR)**, as introduced in our paper:

> **‚ÄúDecoding Dense Embeddings: Sparse Autoencoders for Interpreting and Discretizing Dense Retrieval.‚Äù**  
---

## üó∫Ô∏è Table of Contents
1. [Methodology Overview](#methodology-at-a-glance)
2. [Requirements](#installation)  
3. [SAE Training & Evaluation](#1-sae-training--evaluation)
4. [Generating Latent Concept Description](#2-cl-sr-index-construction)
5. [CL-SR Indexing](#2-cl-sr-index-construction)  
6. [CL-SR Inference & Benchmarking](#3-cl-sr-inference--benchmarking)  
7. [Citation](#citation)  
8. [License](#license)

---

## 1. Methodology Overview
<div align="center">
<img src="https://github.com/user-attachments/assets/dd907ab3-e66d-49fe-930d-56351c7b7c79" width="100%" alt="Framework overview"/>
</div>

Overview of our method. We first **train a SAE** to decompose dense embeddings into latent concepts. Given a query or a passage, the SAE encoder sparsely activates latent concepts, which are **mapped to natural language descriptions** allowing human interpretability tasks. In **CL-SR, queries and passages are represented as sets of activated latent concepts.** We also demonstrate its effectiveness on subsets where traditional sparse retrieval methods struggle.

---

## 2. Requirements

### Setup `python` environment
```
conda create -n CL-SR python==3.10.10
conda activate CL-SR
```

### Install other dependencies
```
pip install -r requirements.txt
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
```

### Download DPR-model embedding

To train the SAE, you need embeddings generated by a DPR model. For this purpose, we use the dense embeddings from SimLM, which has demonstrated strong performance among available DPR models. 
You can download the embeddings for all 8.8 million passages of the MS MARCO passage retrieval dataset, as well as the Dev and TREC-DL 19/20 queies, qrels, from the link below:

---

## 3. SAE Training & Evaluation
```bash
# Train SAE (example: k = 32, MSMARCO train passages)
python sae/train_sae.py \
    --embeddings data/msmarco_train/embed.npy \
    --hidden-mult 32 --k 32 --batch 4096 \
    --lr 5e-5 --epochs 100 \
    --out checkpoints/sae_k32.pt

# Evaluate reconstruction & IR fidelity (Dev, TREC-DL19/20)
python sae/eval_sae.py \
    --sae checkpoints/sae_k32.pt \
    --embeddings data/msmarco_dev/embed.npy \
    --qrels data/trec_dl19/qrels.txt
```
---

## 4. Generating Latent Concept Description

---

## 5. CL-SR Indexing

---

## 6. CL-SR Inference & Benchmarking

---

## 7. Citation

---

## 8. Citation
