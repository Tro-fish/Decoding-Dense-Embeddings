# Decoding Dense Embeddings  
Sparse Autoencoders for Interpretable Dense Retrieval
<!-- Badges are optional. Delete if your repo is private. -->
![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/pytorch-2.x-lightgrey)
![License](https://img.shields.io/badge/license-Apache%202.0-green)

A reference implementation of **Sparse Autoencoder (SAE)‚Äìbased DPR models interpretation** and **Concept-Level Sparse Retrieval (CL-SR)**, as introduced in our paper:

> **‚ÄúDecoding Dense Embeddings: Sparse Autoencoders for Interpreting and Discretizing Dense Retrieval.‚Äù**  
---

## üó∫Ô∏è Table of Contents
1. [Methodology Overview](#methodology-overview)
2. [Requirements](#requirements)
3. [SAE Training & Evaluation](#sae-training--evaluation)
4. [Generating Latent Concept Descriptions](#generating-latent-concept-descriptions)
5. [CL-SR Indexing](#cl-sr-indexing)
6. [CL-SR Inference & Benchmarking](#cl-sr-inference--benchmarking)
7. [Citation](#citation)
8. [License](#license)

---

## Methodology Overview

<div align="center">
<img src="https://github.com/user-attachments/assets/dd907ab3-e66d-49fe-930d-56351c7b7c79" width="100%" alt="Framework overview"/>
</div>

Overview of our method. We first **train a SAE** to decompose dense embeddings into latent concepts. Given a query or a passage, the SAE encoder sparsely activates latent concepts, which are **mapped to natural language descriptions** allowing human interpretability tasks. In **CL-SR, queries and passages are represented as sets of activated latent concepts.** We also demonstrate its effectiveness on subsets where traditional sparse retrieval methods struggle.

---

## Requirements

### Setup `python` environment
```
conda create -n CL-SR python==3.10.10
conda activate CL-SR
```

### Install other dependencies
```
pip install -r requirements.txt
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
```

### Download DPR-model embedding

To train the SAE, you need embeddings generated by a DPR model. For this purpose, we use the dense embeddings from SimLM, which has demonstrated strong performance among available DPR models. 
You can download the embeddings for all 8.8 million passages of the MS MARCO passage retrieval dataset, as well as the Dev and TREC-DL 19/20 queies, qrels, from the link below:

To train the SAE, you need embeddings generated by a DPR model. We use SimLM embeddings for all 8.8 M MSMARCO passages plus the train queries.
Download links: 
This link also provide TREC-DL 19/20, MSMARCO Dev queries, qrels for evaluation.

---

## SAE Training & Evaluation
### Train SAEs
```bash
# Train SAE (example: k = 32, MSMARCO train passages)
python sae/train_sae.py \
    --embeddings data/msmarco_train/embed.npy \
    --hidden-mult 32 --k 32 --batch 4096 \
    --lr 5e-5 --epochs 100 \
    --out checkpoints/sae_k32.pt
```
### Evaluate SAEs
```bash
# Evaluate NMSE
python sae/eval_sae.py \
    --sae checkpoints/sae_k32.pt \
    --embeddings data/msmarco_dev/embed.npy \
    --qrels data/trec_dl19/qrels.txt
```

```bash
# Evaluate Reconstruction Performance
python sae/eval_sae.py \
    --sae checkpoints/sae_k32.pt \
    --embeddings data/msmarco_dev/embed.npy \
    --qrels data/trec_dl19/qrels.txt
```
---

## Generating Latent Concept Descriptions

After training the SAE, we use an LLM to generate a natural-language description for each latent concept.
We use GPT-4.1 Mini api to generate a description for each latent concept.
Using the top 30 most activating passages per latent, this process costs approximately $7.
With this descriptions, we can understand DPR models embeddings and the similarity scoring process.

```
# Example: generate descriptions from top-activating passages
python sae/generate_descriptions.py \
  --sae checkpoints/sae_k32.pt \
  --topK 32 \
  --passages data/msmarco_dev/passages.jsonl \
  --out descriptions/latent32.json
```
---

## CL-SR Indexing

After training the SAE, latent concepts can serve as fundamental retrieval units. We index each passage by its activated latents for efficient retrieval.

```
# Build inverted index of latent concepts (max 24 per doc)
python clsr/build_index.py \
  --sae checkpoints/sae_k32.pt \
  --docs data/msmarco_corpus.tsv \
  --max-latents 24 \
  --index runs/msmarco_latent24/
```
---

## CL-SR Inference & Benchmarking
```
# Search
python clsr/search.py \
  --index runs/msmarco_latent24/ \
  --sae   checkpoints/sae_k32.pt \
  --queries data/msmarco_dev/query.tsv \
  --out runs/dev.rank

# Evaluate
python tools/evaluate_all.py \
  --run runs/dev.rank \
  --hyperparameters 0.6 1.5 2.5
```
---

## Citation

---
## License

---

## License

This project is licensed under the Apache 2.0 License.\
MS MARCO data is provided under Microsoft‚Äôs non-commercial research terms.
